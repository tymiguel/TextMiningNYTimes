{
    "collab_server" : "",
    "contents" : "---\ntitle: \"MA710 - New York Times Articles\"\nauthor: \"Vinay Marwaha, Tyler Miguel, Katelyn Tolbert, and Irene Yang\"\ndate: \"21 Mar 2017\"\noutput:\n  html_document:\n    toc: yes\n---\n\n## Introduction\n\nThe [New York Times](https://www.nytimes.com) (sometimes abbreviated NYT and The Times) is an American daily newspaper, founded and continuously published in New York City since September 18, 1851, by The New York Times Company. The New York Times has won 119 Pulitzer Prizes, more than any other newspaper. The paper's print version in 2013 had the second-largest circulation, behind The Wall Street Journal, and the largest circulation among the metropolitan newspapers in the US. The New York Times is ranked 18th in the world by circulation. \n\nNYT Times exposes APIs ([application programming interfaces](http://en.wikipedia.org/wiki/API)) for noncommercial use allowing developer community to programmatically access New York Times data for use in different applications. With the Times Article Search API, we can search New York Times articles from Sept. 18, 1851 to today, retrieving headlines, abstracts, lead paragraphs, links to associated multimedia and other article metadata.\n\nIn this report, we leverage Times Article Search APIs to search and retrieve news articles based on a unique query term to showcase nuances of text mining techniques in conjunction with `R` programming language for data scientists. Specifically, we categorize the retrieved article content into different clusters to discover the underlying topics per cluster. For the purpose of our report, we focus our analysis on articles containing the word \"drone\" published between June, 1st, 2016 and March, 25th, 2017.\n\n## Prepare the dataset\n\nBefore extracting the articles from the NYT website, we need to load all dependencies such as our required `R` packages and supporting functions that we used in our analysis.\n\n**Required R libraries**\n\nFollowing R libraries are used for the analyses to follow:\n\n* `stringr` for string manipulation\n* `knitr` for creating elegant tables in RMD\n* `ggplot2` for data visualization\n* `plotrix` for drawing pyramid plot\n* `dplyr` for data manipulation and transformation\n* `clValid` for cluster validation tasks\n* `RWeka` for creating n-grams\n* `qdap` for computing word frequencies quickly without creating Document-Term frequency matrix\n* `RWeka` for creating n-grams\n* `tm` for removing stop words\n* `RTextTools` for stemming \n* `wordcloud` for visually representing world cloud\n* `RColorBrewer` for color scheme in the word cloud\n* `syuzhet` to extracts Sentiments from Text\n\n\n```{r warning=FALSE, message=FALSE}\nlibrary(stringr) \nlibrary(knitr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotrix)\nlibrary(clValid)\nlibrary(tm)\nlibrary(RTextTools)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(syuzhet)\nlibrary(qdap)\nlibrary(RWeka)\n```\n\n@team: We do explain the inner workings of the functions. If you look around line 70-75 you will see an explanation of the get.nyt.hits function. Not sure what \"we do not explain the inner working\" is referring too below...\n\nAdditionally, we sourced an `R` script file containing supporting functions which we used in our text mining exercise. We want to recognize that these functions are available at the courtesy of **Professor Oury**. Please note, we do not explain the inner working of these function in this report. To utilize the functions within this `R` script we used the `source` function which accepts name of an `R` script file as an input and loads the `R` functions in the work space from the sourced script in order to be consumed.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\nsource(file = \"20170321_TextMining_functions.R\")\n```\n\nTo better understand the data in our articles, we adjusted the global options of `dplyr.width` to include all columns when printing results to console.\n\n```{r}\noptions(dplyr.width = Inf)\n```\n\nOnce the libraries and functions we need for the analysis are loaded, we extract the articles from the NYT's website using the [NYT Article Search API](https://developer.nytimes.com/).\n\nExtracting articles from NYT is a two-step process. First, we determine number of hits for our search term and then we retrieve actual article content. To get the number of search hits, we set `articlesearch.key` with a unique API key (given to us on request by the NYT's developer website) and then call the `get.nyt.hits()` function which accepts a query term via `query.string` parameter along with a start and end date between which articles of interest were published. \n\nBelow, we pass the query term \"drone\" to the `query.string` to search articles (scope of search includes metadata e.g. keywords, headlines, and body of the articles) which are published between June 1st, 2016 and March 25th, 2017. The function `get.nyt.hits()` concatenates default inputs needed to call the Article Search API along with customizable inputs that specify which articles to search. The Article Search API is invoked and subsequently JSON response is received using `getURL()` function. The JSON response is passed to `fromJSON()` function which converts JSON formatted response into a `R list`. Lastly, from this list we extract number of hits, using `$response$meta['hits']` to determine how many hits there are for the query string with the given beginning and end dates.\n\n```{r, eval=FALSE}\narticlesearch.key <- \"b12db84090d04b5d89566c61ae829cd1\"\nget.nyt.hits(query.string = \"drone\",     \n             begin.date = \"20160601\",   \n             end.date = \"20170325\")    \n```\n\nWe get **1,119** hits for query term \"drone\" and article publishes dated between June 1st, 2016 and March 25th, 2017.\n\nWe can only retrieve 10 articles with each call to Article Search API. Set of 10 articles are grouped into a page with zero indexing system, meaning page 0 contains 1 to 10 articles, page 1 contains 11 to 20 articles, so on and so forth.\n\nWe must make several calls to the Article Search API to extract all 1,119 articles in set of ten per page from the NYT. In addition, since each API is limited to 5 calls per second, the subsequent method call is interrupted for 1 second using ` Sys.sleep(1)`. Function `get.nyt.articles()` wrap these multiple calls into perceived one call with 1 sec sleep logic. The function `get.nyt.articles()` takes the same arguments as `get.nyt.hits()` with one additional argument `pages`. We input `pages=-1` to return all articles with query term \"drone\" published between June 1st, 2016 and March 25th, 2017. Since each page contains ten articles, e.g. if we have 99 articles, we only need to get 0 to 9 pages of articles to retrieve all 99 articles.\n\nThe function `get.nyt.pages()` follows a similar flow as `get.nyt.hits()`, except rather than retrieving the \"hits\" we retrieve the \"docs\" using `$response$docs` from the `R list`. `$response$docs` is then passed to `list.select()` which extracts `headline`, `snippet`, `lead_paragraph`, `abstract`, and `pub_date` fields. Following, the extracted list is passed to `dfrow.from.list` which takes a list of `headline`, `snippet`, `lead_paragraph`, `abstract`, and `pub_date`, unlist it, and then binds the rows together into a single data frame for *each article*. When applied over the entire page, we return a single data frame for with 10 articles. The data frame for each page is then sent back to the `get.nyt.articles()` and combined using the `bind_rows` function to create a single data frame for all the pages and assigned to the object `article.df`. Below function call is to the wrapper function with all the above details hidden in the wrapper function.\n\n```{r, eval=FALSE}\narticle.df <- get.nyt.articles(pages = -1, \n                              query.string = \"drone\",\n                              begin.date   = \"20160601\",\n                              end.date     = \"20170325\") \n```\n\nWe restrict running above code once for the purpose of this report because of following reasons:\n\n1. Each API is limited to 1K calls per day, and 5 calls per second.\n2. Retrieving article content is a time consuming task.\n3. Article data is past data and is not changing.\n\nInstead, we take the data frame of the articles we just extracted from the NYT API, and save the data frame in an `.RData` file. This allows us to load the data that we have already extracted without making additional calls to the NYT API affording us efficiency and productivity.\n\n```{r, eval=FALSE}\nsave(article.df, file = \"drone_article.df.RData\")\n```\n\nThe function `load()` takes the file name of article data set as an input and loads that article data set into the `R` environment for further analysis.\n\n```{r}\nload(file = \"drone_article.df.RData\")\n```\n\n### Inspect drone article data set\n\nWe look at the dimensions of `article.df` using the `dim()` function. The data frame has `r dim(article.df)[1]` rows and `r dim(article.df)[2]` columns. Each row in the `article.df` represents an entire article, and each column represents specific data element of that article.\n\n```{r}\ndim(article.df)\n```\n\nTo begin understanding article data set, we first look at a sample of the first five articles. We see a preview of the headline, a snippet of the article content, and the first few lines of the leading paragraph. Based upon our sample, we did not see any unique topics, but we found it useful to preview the data to get a sense of how it is structured. However, looking at that first 5 headlines, we can see that there is a variety of article descriptions, from recreational drone use rules to CIA drone strikes. We use\n\n```{r}\nnum.articles <- nrow(article.df)\ndoc.ndx <- 1:5 \nvar_df <- article.df[doc.ndx, ]\nkable(var_df, format = \"pandoc\")\n```\n\nThe \"drone\" article data frame has the following columns as shown below:\n\n```{r}\nnames(article.df)\n```\n\nThe column names are self-explanatory in terms of the information about articles they represent. \n\nAs objective of this text mining exercise is to group articles in different categories and discover a distinct topic for each category, we will not consider the publication date column for our analysis.\n\nFor most of the articles, abstracts are not available. Out of the `r nrow(article.df)` articles, `r sum(is.na(article.df$abstract))` articles' abstracts are not available (represented by `NA` in *abstract* column). As a result we drop this column from further analysis.\n\n```{r}\nnum.articles\nsum(is.na(article.df$abstract))\n```\n\nWe investigate random records from *snippet* and *lead_paragraph* columns and observe that information contained in both these columns are identical. As a result, we use only *snippet* for further analysis. \n\nBelow, we select the first five articles from the the article data frame to investigate these 5 articles for content similarities between lead paragraph and snippets.\n\n**Lead paragraph**\n\n```{r}\narticle.df$lead_paragraph[doc.ndx]\n```\n\n**Snippet**\n\n```{r}\narticle.df$snippet[doc.ndx]\n```\n\nSince snippets capture the essence of an article, we do not consider the *headline* column for our analysis. We retrieve the snippets from the article data frame and store them into a vector, `docs`. Once the snippets are retrieved from `article.df`, we remove all `NA`s and snippets which do not contain at least one alphabetic character. We used `str_detect()` function along with a `regexp` pattern. \n\n```{r}\ndocs <- article.df$snippet\ndocs <- docs[-c(which(is.na(docs)), which(!str_detect(docs, c(\"[a-z]\"))))]\n```\n\nAfter cleaning the data, we remove the snippets that do not contain at least one alphabetic character, we use `length(docs)` to check how many articles we have for analysis.\n\n```{r}\nlength(docs)\n```\n\nWe've removed 6 articles, and still have 1,113 left for analysis.\n\n### Clean the snippets\n\nWe noticed that the text contains punctuation. Below, we removed punctuation along with other characters that could affect our analysis using the `clean.documents()` function. The function takes a vector as an input, coverts the text to lower case letters and goes through a series of `regex` replacement expressions that removes apostrophes with an \"s\", dollar signs, and periods, and then converts numbers, punctuation, and white space to a space. In addition, the function trims trailing spaces.\n    \n```{r}\ndocs.clean <- clean.documents(docs)\n```\n\nWe check a the first five articles of the cleaned data set to ensure that all punctuation and other nuances were removed. \n\n```{r}\ndocs.clean[doc.ndx]\n```\n\nWe see that our `clean.documents` function is effective. \n\n### Top terms in snippets\n\nWe look at the top 10 terms in the article snippets before taking on detailed text mining tasks. We use `freq_terms` function available in `dqap` package to identify these words. The idea is to identify stop words which may not have any value to our analysis.\n\n@team: Tyler can't run this chunk\n```{r}\nterm_frequencies <- freq_terms(docs.clean, top = 10, at.least = 3, tm::stopwords(\"english\"))\nggplot(term_frequencies, aes(x = WORD, y = FREQ)) +\n  geom_bar(stat = \"identity\", col = \"tan2\", fill = \"tan2\") +\n  scale_x_discrete(limits = rev(term_frequencies$WORD)) +\n  xlab(\"Words\") +\n  ylab(\"Frequencies\") +\n  coord_flip() \n```\n\nFrom the bar plot, it is evident that the words \"said\" and \"drones(s)\" are over-represented. Since \"drone(s)\" is our query term we expect this word the have a greater frequency than most. In our analysis we remove these identified words by adding them as \"stop words\".\n\n## Base investigation\n\nWe choose and text-mine the \"snippets\" of articles to identify district clusters and topics across article data set. Following parameters are used for base investigation.\n\nParameter | Value | Reason\n--------- | ----- | ------\nQuery term | \"drone\" | Interested in application of drones\nBegin date | 1 Jun 2016 | Interested in 2016 articles\nEnd date   | 25 Mar 2017 | Interested in 2017 articles\nField      | `snippet` | Best representation of articles\nStemming   | Yes | Improve \"likeness\" between documents\nN-grams    | 1 | Consider each word between spaces a single word\nStopwords  | \"english\" | Remove english stop words\nStopwords  | \"said\", \"drone\" | These include the search term\nWeighting  | Tf | Process to find \"like\" documents\nThreshold  | 5 | Filter out least important words\n`k`        |  | To be decided using cluster validation procedure\n\n### Modify the words in snippets using stemming, n-grams and stopwords\n\nIn our first iteration, in an attempt to find meaning within our data set, we use the `modify.words()` function to modify the words in the document, stemming the snippets and removing stop words. \n\nStemming identifies the root of a word, and where possible, reclassifies the word to be the root version. For example, the root of happiness is \"happy/happi\", therefore stemming ensures that most variations of happy/happiness/happier/happiest are classified under the same root term. \n\nStop words are common English words such as \"and\", \"or\", \"the\", and \"a\". It is useful to remove these stop words so that the main concepts and themes of the articles are more apparent.\n \n```{r}\ndocs.sns <- \n  modify.words(\n    docs.clean,  \n    stem.words = TRUE,  \n    ngram.vector = 1, \n    stop.words=\n      c(stopwords(kind = \"english\"),\n        \"said\", \"drone\"\n      )\n  )\n```\n\nNow that we have modified the data frame containing the documents, we can see the results.\n\n```{r}\ndocs.sns[doc.ndx]\n```\n\nThe snippets have been transformed using the `modify.words()` function. \n\n### Create the document matrix\n\nWe utilize term frequency as our weighing option. Since we have already stemmed the words, and removed punctuation, we will change these parameters to `FALSE`. \n\n```{r}\ndoc.matrix <- \n  create_matrix(docs.sns, \n                language = \"english\",      \n                stemWords = FALSE,         \n                removePunctuation = FALSE, \n                weighting = tm::weightTf   \n  )\n```\n\nCreate the document-term matrix and check the dimension of matrix. The output of the `doc.matrix()` function is an object of class \"DocumentTermMatrix\". To convert this object into a matrix of the documents and terms, we use the `as.matrix()` function and store the result in `dtm`.\n\n```{r}\ndtm <- as.matrix(doc.matrix)\ndim(dtm)\n```\n\nCheck the number of words in the document term matrix. There are `r nrow(dtm)` articles and `r ncol(dtm)` words in our matrix.\n\nMany of the words in our matrix are not present in several of the snippets. Continuing our analysis, we keep the words that occur at least 5 times, indicated by the `freq.threshold` parameter. This will not only filter out least important words, but also make our analysis more manageable and improve interpretation.\n \n```{r}\ndtm <- reduce.dtm(dtm, freq.threshold = 5)\nncol(dtm)\n```\n\nWe have reduced our word count from 3,869 to 907. \n\nWe start with some basic analysis by looking at the frequency of top 10 words in the article snippets. Calling `colSums()` on newly made matrix aggregates all the terms used in snippets. Once we have the `colSums()`, we sort() them with `decreasing = TRUE`, so we can focus on the most common terms.\n\nLastly, you can make a bar plot of the top 10 terms of term_frequency.\n\n```{r}\nterm_frequency <- colSums(dtm)\nterm_frequency <- sort(term_frequency, decreasing = TRUE)\nterm_frequency[1:10]\nbarplot(term_frequency[1:10], col = \"tan\", las = 2)\n```\n\n@team: I thought we removed drone? Why is it here? Also, said is not here, why does the description below say it is? Things to do: See why drone is here, and then edit explanation below. Also, I thought we were using a `Tf` weighting, where is the `TfIdf` weighting above?\n\nIt is seen that word \"drone\" and \"said\" are over represented. Over representation of \"drone\" is expected as it is the query term.  Since we are using `TfIdf` weighing option we do not remove this word as `TfIdf` mitigate the over-representation of most common words. But over-represented words must be removed when `Tf` weighing is used as we do in subsequent iteration of our investigation. We also visualize the the term frequency using word cloud of top 50 words.\n\n@team: I think we should change the color scheme below (or maybe just the red). Specifically, this type of \"red\" indicates a feeling of power/stop/bad/anger, which isn't what we are going for here. Where as it seems like we are just interested in designating difference colors. Take a look at `?RColorBrewer`. It good base palettes.\n\n(old color scheme if we want to revert it... c(\"grey80\", \"darkgoldenrod1\", \"tomato\" ).)\n\n```{r}\nword_freqs <- data.frame(term = names(term_frequency), num = term_frequency)\nwordcloud(word_freqs$term, word_freqs$num, max.words = 50, \n          colors =  brewer.pal(3, \"Blues\"))\n```\n\nThe word cloud above shows the term-frequencies of all the terms in the article snippets. The size and color of the word is relative to the number of times the word shows up in the snippets. Therefore, we can see that the words \"state\" and \"kill\" are the most represented in our snippets. Currently, this doesn't give us much information, however, it will be something to note throughout the analysis.\n\nNext, we utilize cluster validation function to find the optimal size of clusters using two methods - `kmeans` and `pam`. The \"internal\" validation measures Connectivity, Dunn, Silhouette scores for the. The default maximum number of items which can be clustered are 600. For this report we use all the available observations after the data is cleaned. We evaluate 2 to 8 clusters and choose the one which has optimal scores for further analysis. We store the validation results in the object `intern` and print the summary of measured validations. The output summarizes Connectivity, Dunn, and Silhouette scores both for k-means and PAM methods for each of the cluster sizes.\n\nWe look at the optimal scores and plot the results to choose an optimal cluster size for further analysis. We use `optimalScores` method to get an optimal cluster size based on the Connectivity, Dunn, and Silhouette scores. The plot below is a visual representation of the optimal scores.\n\n@team: clValid is part random so I set seed and continued down the line. \n\n```{r}\nset.seed(101)\nintern <- clValid(dtm, 2:8, clMethods = c(\"kmeans\", \"pam\"), \n                  validation = \"internal\", maxitems = +Inf)\nsummary(intern)\nplot(intern)\noptimalScores(intern)\n```\n\nOptimal clusters are chosen based on lowest connectivity, highest Dunn index, and greatest Silhouette width. Based on these measures optimal cluster size is 2 using `kmeans` method.\n\nWe first try the analysis using `kmeans` with two centers.\n\n```{r}\nk <- 2\nset.seed(113)\ncluster = kmeans(dtm, k)$cluster\n```\n\nWe evaluate the clusters using `table` to check the cluster sizes.\n\n```{r}\nas.data.frame(table(cluster))\n```\n\nWe plot clusters using the first 2 principal components.\n\n@team: this process below is interesting, why did we use the first 2 Principal components? What's the reason? (I think I understand, but would love more of an explanation.)\n\n```{R}\nset.seed(121)\nprcomp <- prcomp(dtm)$x[, 1:2]\nprcomp_df <- as.data.frame(prcomp)\nprcomp_df <- bind_cols(prcomp_df, data.frame(\"Clusters\" = factor(cluster)))\nggplot(prcomp_df, aes(x = PC1, y = PC2, col = Clusters)) +\n  geom_point(shape = 1, size = 2, position = position_jitter(width = .1), alpha = .6) \n```\n\nAs we can see, using the first two principal components to reduce the dimensions of the data, we can see a distinct difference in the clusters which appear to be more sensitive to the first principal component.\n\n### Evaluate Clusters\n\nNext we evaluate the clusters using `check.cluster` to look at the common words in each cluster. The `check.cluster` function takes a cluster object and a minimum number of rows that a cluster must have to be displayed and internally passes this information to the `TopWords` function where `TopWords` is performed on each cluster. The final result is then combined using the `bind_cols` function and returned.\n\n```{r warning=FALSE}\ncheck.clusters(cluster, 5) \n```\n\nWe can see that 11% of the terms in the 866 articles in cluster 1 contain the word new. Additionally, examining cluster 2 it appears that the cluster could be a negative cluster related to violence, specifically, because of the frequency of the words \"kill\", \"milit\", \"attack\", and \"strike\".\n\nThen, we evaluate the clusters by looking at the first 5 documents in each cluster.\n\n```{R}\nfor(i in 1:k) {\n  cat(\"\\n\")\n  cat(\"\\n\")\n  print(paste(\"Cluster : \", i))\n  cat(\"\\n\")\n  print(head(view.cluster(i), 5))  \n}\n\n```\n\nFrom cluster 2, we notice that the headlines of the articles appear to be more violent then cluster 1, given by the headlines that indicate drone strikes and attacks.\n\nFinally, we visualize common words in the derived clusters. First, we separate the two clusters, taking the first 50 snippets, and store them in different objects. Then we collapse all the snippets into one large snippet making a single text vector element. Following, we combine two text elements into a single object of length 2. From there, we clean the snippets, stem the words, and remove stop words. With the cleaned data, we create a term-document matrix and set the column names for each of the documents to Cluster 1 and Cluster 2 and create a word cloud of common words in both the clusters using `commonality.cloud` function.\n\n@team: I changed the color scheme\nOld color scheme if we want to revert: \"steelblue1\"\n\n```{r warning=FALSE}\ncluster1 <- view.cluster(1)[1:50]\ncluster2 <- view.cluster(2)[1:50]\nall.cluster1 <- paste(cluster1, collapse = \" \")\nall.cluster2 <- paste(cluster2, collapse = \" \")\nall.cluster <- c(all.cluster1, all.cluster2)\nall.clean <- clean.documents(all.cluster)\nall.sns = \n  modify.words(\n    all.clean,  \n    stem.words=TRUE,  \n    ngram.vector=1, \n    stop.words=\n      c(stopwords(kind=\"english\"), \"said\", \"drone\", \"drones\")\n  )\nall.tdm <- TermDocumentMatrix(VCorpus(VectorSource(all.sns)))\ncolnames(all.tdm) <- c(\"Cluster 1\", \"Cluster 2\")\n\nall.matrix <- as.matrix(all.tdm)\ncommonality.cloud(all.matrix, colors = brewer.pal(3,\"Blues\"), max.words = 100)\n```\n\nThe most common words across the clusters are words such as \"report\", \"crash\", and \"target\".\n\nNext, we visualize distinct words in the derived clusters. We use `comparison.cloud` function to achieve this.\n\n```{r warning=FALSE}\ncomparison.cloud(all.matrix, colors =c(\"darkorange\", \"darkred\") , max.words = 50)\n```\n\nCluster 2 is starting to take shape as a cluster that represents articles that are related to violent and war-related articles. Cluster 1, we can see, appears to be more neutral with key words that indicate personal use of drones given the words \"photograph\", \"video\", and \"lawmak\".\n\nWe also visualize the difference in frequencies of top 25 common words between these clusters using pyramid plot. To do this, we set up our data by creating a subset containing only the common words between the two clusters, computing the absolute difference between the frequencies of common words between two clusters, and combining the `common_words` data frame and the `difference` vector. Then we order the data frame in descending order of differences, and extract the top 25 words and store them in a data frame.\n\nFinally, we create the pyramid plot using the `pyramid.plot` function. \n\n```{r message=FALSE}\ncommon_words <- subset(all.matrix, all.matrix[, 1] > 0 & all.matrix[, 2] > 0)\ndifference <- abs(common_words[, 1] - common_words[, 2])\ncommon_words <- cbind(common_words, difference)\ncommon_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]\ntop25_df <- data.frame(x = common_words[1:25, 1], \n                       y = common_words[1:25, 2], \n                       labels = rownames(common_words[1:25, ]))\npy.par <- pyramid.plot(top25_df$x, top25_df$y,\n             labels = top25_df$labels, gap = .8,\n             top.labels = c(\"Cluster 1\", \"Words\", \"Cluster 2\"),\n             main = \"Words in Common\", laxlab = NULL, \n             raxlab = NULL, unit = NULL)\n```\n\nWe can see that, although \"attack\" is in both cluster, it is more prevalent in the second cluster. Additionally, we can see that \"use\" is far more prevalent in the first cluster.\n\n### Sentiment Analysis\n\nIn order to evaluate clusters, we conduct the sentiment analysis on these clusters.\n\n**Emotion**\n\nTo begin with, we use the `syzhet` library to conduct the sentiment analysis. This library contains four sentiment dictionaries that separate the emotion into 8 emotions (anger, joy, trust, and etc.) and 2 sentiments (positive and negative). \n\n**Data preparation**\n\nWe assign two clusters to two different `R` objects. Cluster 1 has 872 snippets and cluster 2 has `r length(view.cluster(2))` snippets.\n\n```{r}\ncluster1 <- view.cluster(1)\ncluster2 <- view.cluster(2)\n```\n\n**Sentiment analysis for cluster 1**\n\nThe `get_nrc_sentiment` function implements Saif Mohammadâ€™s NRC emotion lexicon (a large word list) and this function returns a data frame in which each row represents a sentence from the original text. \n\n```{r}\nsenti1 <- get_nrc_sentiment(cluster1)\nsentiment_cr1_senti <- cbind(cluster1, senti1)\n```\n\nThere are `r nrow(sentiment_cr1_senti)` observations in the resulting data frame which is equal to the number of snippets in cluster 1. \n\n**Vistualize Emotions**\n\nWe check how many words fall into each categories of emotion and visualize the results.\n\n*Emotion*\n\n```{r}\ntotalcount_word1 <- data.frame(sentiment_cr1_senti[, c(2:11)])\n\nbarplot(\n  sort(colSums(totalcount_word1[, 1:8])), \n  cex.names = 0.6, \n  las = 1, \n  main = \"Emotions in Cluster 1\"\n)\n\n```\n\nFrom the data we see that cluster 1 has most number of `r names(sort(colSums(totalcount_word1[, 1:8]))[8])` words, followed by `r names(sort(colSums(totalcount_word1[, 1:8]))[7])` and `r names(sort(colSums(totalcount_word1[, 1:8]))[6])`. \n\n*Polarity*\n\nWe plot the polarity of cluster 1. In our case, polarity represents the the positive and negative emotions in the articles.\n\n```{r}\nbarplot(\n  sort(colSums(totalcount_word1[, 9:10])), \n  cex.names = 0.7, \n  las = 1, \n  main = \"Polarity in CLuster 1\",  xlab = \"Count\"\n)\n```\n\nOverall, the cluster 1 has more `r names(sort(colSums(totalcount_word1[, 9:10]))[2])` words than `r names(sort(colSums(totalcount_word1[, 9:10]))[1])`. \n\n**Sentiment Analysis For Cluster 2**\n\nWe repeat the same process for cluster 2. \n\n```{r}\nsenti2 <- get_nrc_sentiment(cluster2)\nsentiment_cr2_senti <- cbind(cluster2, senti2)\n```\n\n**Vistualize Emotions**\n\nWe check how many words fall into each categories of emotion and visualize the results.\n\n*Emotion*\n\n```{r}\ntotalcount_word2 <- data.frame(sentiment_cr2_senti[, c(2:11)])\n\nbarplot(\n  sort(colSums(totalcount_word2[, 1:8])), \n  cex.names = 0.6, \n  las = 1, \n  main = \"Emotions in cluster 2\"\n  )\n```\n\nWe see that cluster 2 has most number of `r names(sort(colSums(totalcount_word2[, 1:8]))[8])` words, followed by `r names(sort(colSums(totalcount_word2[, 1:8]))[7])` and `r names(sort(colSums(totalcount_word2[, 1:8]))[6])`.  \n\nAs we have examined before, we can see that fear is largely represented in these articles. This is consistent with our analysis that cluster 2 appears to be more violent in nature than cluster 1.\n\n*Polarity*\n\n```{r}\nbarplot(\n  sort(colSums(totalcount_word2[, 9:10])), \n  cex.names = 0.7, \n  las = 1, \n  main = \"Polarity in cluster 2\", xlab = \"Count\"\n  )\n```\n\nOverall, the cluster 2 has more `r names(sort(colSums(totalcount_word2[, 9:10]))[2])` words than `r names(sort(colSums(totalcount_word2[, 9:10]))[1])`. \n\nWe conclude that article data set in general have negative words, which is consistent with our analysis.\n\n## Investigation 2\n\nFor our second investigation, we used the same article data and parameters, except now we will change the weighting parameter to the inverse-document frequency.\n\nParameter | Value | Reason\n--------- | ----- | ------\nQuery term | \"drone\" | Interested in application of drones\nBegin date | 1 Jun 2016 | Interested in 2016 articles\nEnd date   | 25 Mar 2017 | Interested in 2017 articles\nField      | `snippet` | Best representation of articles\nStemming   | Yes | Improve \"likeness\" between documents\nN-grams    | 1 | Consider each word between spaces a single word\nStopwords  | \"english\" | Remove english stop words\nStopwords  |  \"said\", \"drone(s)\" | These include the search term\nWeighting  | TfIdf | Process to find how important a word is to a document\nThreshold  |  | Not filter out least important words\n`k`        |  | To be decided using cluster validation procedure\n\n\n### Modify the words in snippets using stemming, n-grams and stopwords\n\nIn our first iteration, to attempt to find meaning within our data set, we use the `modify.words()` function to modify the words in the document using the stemming and removing stop words. \n\n```{r}\ndocs.sns <- \n  modify.words(\n    docs.clean,  \n    stem.words = TRUE,  \n    ngram.vector = 1, \n    stop.words =\n      c(stopwords(kind=\"english\"), \"said\", \"drone\"  \n      )\n  )\n```\n\nNow that we have modified the data frame containing the documents, we can see the results.\n\n```{r}\ndocs.sns[1:5]\n```\n\nThe snippets have been transformed using the `modify.words()` function.\n\n### Create the document matrix\n\nWe utilize term frequency-inverse document frequency (`TfIdf`) as our weighing option.\n`TfIdf` measures the uniqueness of a term in the news snippets. If a term shows up only in 10% of the documents then it is unique. If a term shows up in 90% of the documents then it is not all that unique. It indicates the importance of the term that appears in 10% of documents and places a higher weight on a term that appears multiple times in a specific document but do not appear in many documents. \n\n\n```{r}\ndoc.matrix <- \n  create_matrix(docs.sns, \n                language = \"english\",     \n                stemWords = FALSE,         \n                removePunctuation = FALSE, \n                weighting=tm::weightTfIdf \n  )\n```\n\nCreate the document-term matrix and check the dimension of matrix.\n\n```{r}\ndtm <- as.matrix(doc.matrix) \ndim(dtm)\n```\n\nCheck the number of words in the document term matrix. There are `r ncol(dtm)` words.\n\n```{r}\nncol(dtm)\n\n```\n\nWe normalize the matrix because it improves the clustering using Euclidean distance. The `norm_eucl` function takes matrix as an input and for each document normalize all the words selected from the last step. \n\n```{r}\nnorm_eucl <- function(m) m/apply(m, MARGIN = 1, FUN = function(x) sum(x^2)^.5)\ndtm <- norm_eucl(dtm)\n```\n\nWe look at the optimal scores and plot results to choose the optimal cluster size for further analysis. We use `optimalScores` method to get optimal cluster size based on Connectivity, Dunn, and Silhouette scores. We use plot to obtain a visual representation of optimal scores. \n\n```{r}\nset.seed(130)\nintern <- clValid(dtm, 2:8, clMethods = c(\"kmeans\", \"pam\"), \n                  validation = \"internal\", maxitems = +Inf)\nsummary(intern)\nplot(intern)\noptimalScores(intern)\n```\n\nOptimal clusters are chosen based on lowest connectivity, highest Dunn index, and greatest Silhouette width. Based on connectivity and Dunn measures optimal cluster size is 2 using `kmeans` and based on Silhouette measure optimal cluster size is `r as.integer(as.character(optimalScores(intern)$Clusters[3]))` using `kmeans` method.\n\nWe do analysis using `kmeans` with two centers.\n\n```{r}\nk <- 2\nset.seed(140)\ncluster <- kmeans(dtm, k)$cluster\n```\n\nWe evaluate the clusters using `table` to check the cluster sizes\n\n```{r}\nas.data.frame(table(cluster))\n```\n\nWe plot clusters using the first 2 principal components.\n\n```{R}\nset.seed(150)\nprcomp <- prcomp(dtm)$x[, 1:2]\nprcomp_df <- as.data.frame(prcomp)\nprcomp_df <- bind_cols(prcomp_df, data.frame(\"Clusters\" = factor(cluster)))\nggplot(prcomp_df, aes(x = PC1, y = PC2, col = Clusters)) +\n  geom_point(shape = 1, size = 2, position = position_jitter(width = .1), alpha = .8) \n```\n\nIt appears that our clusters are more sensitive to the values in principal component 2, rather than principal component 1, like our first investigation. \n\n### Evaluate Clusters\n\nNext we evaluate the clusters using `check.cluster` to look at the common words in each cluster. Again, the second parameter is the minimum number of rows that a cluster must have to be displayed.\n\n```{r warning=FALSE}\ncheck.clusters(cluster,5) \n```\n\nFinally, evaluate the clusters by looking at the documents in the clusters\n\n```{R}\nfor(i in 1:k) {\n  cat(\"\\n\")\n  cat(\"\\n\")\n  print(paste(\"Cluster : \", i))\n  cat(\"\\n\")\n  print(head(view.cluster(i), 5))  \n}\n```\n\n@team: why did we choose 8? I can make up reasoning here but I'm not sure if someone already has some. Also, from the PC plot, it seems that a DBSCAN clustering would be more informative because it looks like our clusters are density based.\n\nNext, we do analysis using `kmeans` with eight centers.\n\n```{r}\nk <- as.integer(as.character(optimalScores(intern)$Clusters[3]))\nset.seed(160)\ncluster <- kmeans(dtm, k)$cluster\n```\n\nWe evaluate the clusters using `table` to check the cluster sizes\n\n```{r}\nas.data.frame(table(cluster))\n```\n\nWe plot clusters using the first 2 principal components.\n\n```{R}\nset.seed(180)\nprcomp <- prcomp(dtm)$x[, 1:2]\nprcomp_df <- as.data.frame(prcomp)\nprcomp_df <- bind_cols(prcomp_df, data.frame(\"Clusters\" = factor(cluster)))\nggplot(prcomp_df, aes(x = PC1, y = PC2, col = Clusters)) +\n  geom_point(shape = 1, size = 2, position = position_jitter(width = .1), alpha = .8) \n```\n\nNext we evaluate the clusters using `check.cluster` to look at the common words in each cluster.The second parameter is the minimum number of rows that a cluster must have to be displayed.\n\n```{r warning=FALSE}\nkable(check.clusters(cluster,5) , format = \"pandoc\")\n```\n\nFinally, evaluate the clusters by looking at the documents in the clusters.\n\n```{R}\nfor(i in 1:k) {\n  cat(\"\\n\")\n  cat(\"\\n\")\n  print(paste(\"Cluster : \", i))\n  cat(\"\\n\")\n  print(head(view.cluster(i), 3))  \n}\n```\n\nIf we look at cluster 1, we will see that the headlines of the articles are all the same indicating that this article headline appears to be a standard headline for a reoccurring news release that likely happens daily and includes a summary of articles on the New York Times website.\n\n## Investigation 3\n\nFor investigation 3, we used the same article data and parameters, except now we will change the weighting parameter back to term frequency and we will also increase our N-grams to 2. Additionally, we will adjust our threshold to 3.\n\nParameter | Value | Reason\n--------- | ----- | ------\nQuery term | \"drone\" | Interested in application of drones\nBegin date | 1 Jun 2016 | Interested in 2016 articles\nEnd date   | 25 Mar 2017 | Interested in 2017 articles\nField      | `snippet` | Best representation of articles\nStemming   | No | To increase specificity of word selection\nN-grams    | 2 | Consider each word between spaces a single word, and each adjacent word combination\nStopwords  | \"english\" | Remove english stop words\nStopwords  |  \"said\", \"drone(s)\"\" | These include the search term.\nWeighting  | Tf | Process to find \"like\" documents\nThreshold  |  3 | To filter out least important words\n`k`        |  | To be decided using cluster validation procedure\n\n\n### Modify the words in snippets using stemming, n-grams and stopwords\n\nIn our first iteration, to attempt to find meaning within our data set, we used the `modify.words()` function to modify the words in the document using the stemming, n-grams and removing stop words. \n\nN-grams looks for combinations of words that frequently appear together. A \"2-gram\" creates pairs of two words. For our initial pass through, we only looked for repeating patterns of single words, or pairs of words. \n\n```{r}\ndocs.sns <- \n  modify.words(\n    docs.clean,  \n    stem.words = FALSE,  \n    ngram.vector = 1, \n    stop.words =\n      c(stopwords(kind=\"english\"), \"said\", \"drone\", \"drones\"  \n      )\n  )\n```\n\nNow that we have modified the data frame containing the documents, we can see the results.\n\n```{r}\ndocs.sns[1:5]\n```\n\nThe snippets have been transformed using the `modify.words()` function.\n\n### Create the document matrix\n\nWe create n-gram tokenizer using `Weka` package. Since we consider only 2-grams in the resulting terms, we set both `min` and `max` to 2 in `Weka_control` of `NGramTokenizer`. Once tokenizer is created we pass the tokenizer to `control` of `DocumentTermMatrix` function of `tm` library to create Document-Term frequencies. To use this function we create `VCorpus` from `VectorSource` which take character vector `docs.sns` as an input. We utilize term frequency as our weighing option.\n\n@team: Tyler can't run this chunk\n```{r}\ntokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\ndoc.matrix <- DocumentTermMatrix(VCorpus(VectorSource(docs.sns)), \n                                 control = list(tokenizer = tokenizer, \n                                                weighting = weightTf))\n```\n\nCreate the document-term matrix and check the dimension of matrix.\n\n```{r}\ndtm <- as.matrix(doc.matrix) \ndim(dtm)\n```\n\nCheck the number of words in the document term matrix. There are `r ncol(dtm)` words.\n\n```{r}\ncolCount <- ncol(dtm)\ncolCount\n```\n\nWe noted that we have `r ncol(dtm)` in our matrix. Many of the words are not present in several of the snippets. For our further analysis, we keep the words that occurs at least 5 times indicated by `freq.threshold` parameter. This will not only filter out least important words, but also make our analysis manageable.\n\n \n```{r}\ndtm <- reduce.dtm(dtm, freq.threshold = 3)\n```\n\nCheck the number of columns/words remaining in the document-term matrix. We saw that we reduced our word count from `r colCount` to `r ncol(dtm)`. \n\n```{r}\nncol(dtm)\n```\n\nWe start with some basic analysis by looking at the frequency of top 10 word in the article snippets.Calling `colSums()` on newly made matrix aggregates all the terms used in snippets. Once we have the `colSums()`, we sort() them with `decreasing = TRUE`, so we can focus on the most common terms.\n\nLastly, you can make a bar plot of the top 10 terms of term_frequency.\n\n```{r}\nterm_frequency <- colSums(dtm)\nterm_frequency <- sort(term_frequency, decreasing = TRUE)\nterm_frequency[1:10]\nbarplot(term_frequency[1:10], col = \"tan\", las = 2)\n```\n\nIt is seen that word \"drone\" and \"said\" are over represented. Over representation of \"drone\" is expected as it is the query term.  Since we are using `TfIdf` weighing option we do not remove this word as `TfIdf` mitigate the over-representation of most common words. But over-represented words must be removed when `Tf` weighing is used as we do in subsequent iteration of our investigation. We also visualize the the term frequency using word cloud of top 50 words.\n\n```{r}\nword_freqs <- data.frame(term = names(term_frequency), num = term_frequency)\nwordcloud(word_freqs$term, word_freqs$num, max.words = 50, \n          colors = c(\"grey80\", \"darkgoldenrod1\", \"tomato\"))\n```\n\nWe look at the optimal scores and plot results to choose optimal cluster size for further analysis. Use `optimalScores` method to get optimal cluster size based on Connectivity, Dunn, and Silhouette scores. Use plot to obtain a visual representation of optimal scores. \n\n\n```{r}\nintern <- clValid(dtm, 2:8, clMethods = c(\"kmeans\", \"pam\"), \n                  validation = \"internal\", maxitems = +Inf)\nsummary(intern)\nplot(intern)\noptimalScores(intern)\n```\n\nOptimal clusters are chosen based on lowest connectivity, highest Dunn index, and greatest Silhouette width. Based on connectivity measure optimal cluster size is 2 using `pam` method and based on Dunn and Silhouette measures optimal cluster sizes are `r as.integer(as.character(optimalScores(intern)$Clusters[2]))` and `r as.integer(as.character(optimalScores(intern)$Clusters[3]))` respectively using `kmeans` method.\n\nWe do analysis using `kmeans` with five centers.\n\n```{r}\nk <- 5\ncluster <- kmeans(dtm, k)$cluster\n```\n\nWe evaluate the clusters using `table` to check the cluster sizes\n\n```{r}\nas.data.frame(table(cluster))\n```\n\nWe plot clusters using the first 2 principal components.\n\n```{R}\nprcomp <- prcomp(dtm)$x[, 1:2]\nprcomp_df <- as.data.frame(prcomp)\nprcomp_df <- bind_cols(prcomp_df, data.frame(\"Clusters\" = factor(cluster)))\nggplot(prcomp_df, aes(x = PC1, y = PC2, col = Clusters)) +\n  geom_point(shape = 1, size = 2, position = position_jitter(width = .1), alpha = .8) \n```\n\n### Evaluate Clusters\n\nNext we evaluate the clusters using `check.cluster` to look at the common words in each cluster.The second parameter is the minimum number of rows that a cluster must have to be displayed.\n\n```{r warning=FALSE}\nkable(check.clusters(cluster,5), format = \"pandoc\")\n```\nFinally, evaluate the clusters by looking at the documents in the clusters\n```{R}\nfor(i in 1:k) {\n  cat(\"\\n\")\n  cat(\"\\n\")\n  print(paste(\"Cluster : \", i))\n  cat(\"\\n\")\n  print(head(view.cluster(i), 5))  \n}\n```\n\n## Conclusion\n\n## References\n\n1. https://en.wikipedia.org/wiki/The_New_York_Times\n2. http://developer.nytimes.com/article_search_v2.json#/Documentation/GET/articlesearch.json",
    "created" : 1491589710208.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3173586461",
    "id" : "68FED859",
    "lastKnownWriteTime" : 1491789199,
    "last_content_update" : 1491789199859,
    "path" : "~/Desktop/Bentley/Data Mining (MA710)/Text Mining NY Times/Assignment_3-Apr9-10pm.Rmd",
    "project_path" : "Assignment_3-Apr9-10pm.Rmd",
    "properties" : {
        "ignored_words" : "th\n"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}