{
    "collab_server" : "",
    "contents" : "# New York Times article search\n\nlibrary(bitops)\nlibrary(RCurl)\nlibrary(RJSONIO)\nlibrary(rlist)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(RTextTools)\nlibrary(tm)\nlibrary(ngram)\n\noptions(dplyr.print_min=100)\n\n## Define functions\n#\n\n# Create a single row data frame from a list\ndfrow.from.list = function(aList) { \n  data.frame(rbind(unlist(aList)),\n             stringsAsFactors=FALSE)\n}\n\n# Return: number of hits for a query.string with begin.date and end.date\nget.nyt.hits = function(query.string=\"\", # single word only (REQUIRED)\n                        begin.date=\"\",   # yyyymmdd (REQUIRED)\n                        end.date=\"\"      # yyyymmdd (REQUIRED)\n) { \n  str_c(# create query string to send to NYT\n        \"http://api.nytimes.com\", \n        \"/svc/search/v2/articlesearch.json\",\n        \"?api-key=\",    articlesearch.key,\n        \"&q=\",          str_replace_all(query.string,\" \",\"%20\"),\n        \"&begin_date=\", begin.date,\n        \"&end_date=\",   end.date\n  ) %>%\n  getURL() %>%             # retreive data from NYT\n  fromJSON() %>%           # convert from JSON to a list\n  { .$response$meta['hits'] }\n}\n\nget.nyt.page = function(page=0,          # page number (default: 0) # -1 is all pages\n                        query.string=\"\", # single word only (REQUIRED)\n                        begin.date=\"\",   # yyyymmdd (REQUIRED)\n                        end.date=\"\"      # yyyymmdd (REQUIRED)\n) { \n  str_c(# create query string to send to NYT\n        \"http://api.nytimes.com\", \n        \"/svc/search/v2/articlesearch.json\",\n        \"?api-key=\",    articlesearch.key,\n        \"&q=\",          str_replace_all(query.string,\" \",\"%20\"),\n        \"&begin_date=\", begin.date,\n        \"&end_date=\",   end.date,\n        \"&page=\",       page\n  ) %>%\n  {Sys.sleep(1); .} %>%    # wait 1s (rate limit of 5 requests per second)\n  getURL() %>%             # retreive data from NYT\n  fromJSON() %>%           # convert from JSON to an R list\n  { .$response$docs } %>%  # retrieve only the documents\n  list.select(             # keep only these four fields\n      headline=as.character(headline[\"main\"]), \n      snippet, \n      lead_paragraph, \n      abstract,\n      pub_date) %>% \n  lapply(dfrow.from.list) %>% # convert each list item to a dataframe\n  bind_rows                # create a single dataframe\n}\n\n# pulls documents\nget.nyt.articles = function(pages=0,         # vector of page numbers\n                            query.string=\"\", # single word only (REQUIRED)\n                            begin.date=\"\",   # yyyymmdd (REQUIRED)\n                            end.date=\"\"      # yyyymmdd (REQUIRED)\n) { \n  if (pages[1] == -1) { \n    pages = 0:floor(get.nyt.hits(query.string=query.string,\n                                 begin.date=begin.date, \n                                 end.date=end.date) / 10)\n  }\n  lapply(pages,\n         get.nyt.page, \n         query.string=query.string,\n         begin.date=begin.date,\n         end.date=end.date\n         ) %>%  \n    bind_rows()\n}\n\n# Clean documents\n# every document vector is a character string\n# uses regex\nclean.documents = function (document.vector) {\n  document.vector %>% # document.vector = docs[93:94]\n    tolower() %>%                           # change to lower case\n    str_replace_all(\"'s\",\"\")            %>% # remove \"'s\"\n    str_replace_all(\"’s\",\"\")            %>% # remove \"’s\"\n    str_replace_all(\"\\\\$\",\"\")           %>% # remove dollar signs\n    str_replace_all(\"\\\\.\",\"\")           %>% # remove periods\n    str_replace_all(\"[[:digit:]]+\",\" \") %>% # change numbers to a space\n    str_replace_all(\"[[:punct:]]\",\" \")  %>% # change punctuation to a space\n    str_replace_all(\"[[:blank:]]+\",\" \") %>% # change white space to a space\n    str_trim(side = \"both\")                 # remove spaces at the ends\n}\n\n# Create strings of n-grams\n# takes a clean document vector\nmodify.words = function(document.vector, \n                        stem.words=FALSE, \n                        ngram.vector=1, \n                        stop.words=c()) {\n  document.vector %>% # document.vector = docs.clean\n    str_split(\"[[:space:]]\") %>%  # split on spaces, and return a string character vector          \n    lapply(function(x) setdiff(x,stop.words)) %>% # stems the words\n    { if(stem.words) lapply(., wordStem) \n      else . \n    } %>% \n    lapply(function(x) {  # finds ngrams\n      ngrams(x,ngram.vector) %>%\n        lapply( function(x) paste(x,collapse=\".\")) %>% \n        paste(collapse=\" \") \n    })\n}\n\n# create.ngrams.remove.stopwords = function(document.vector, ngram.vector, stop.words) {\n#   document.vector %>% # document.vector = docs.clean\n#     str_split(\"[[:space:]]\") %>%            # string to vector of words\n#     lapply(wordStem) %>%                    # stem words (OPTION)\n#     lapply(function(word.vector) {          # for each word.vector\n#       word.vector %>%\n#         match(stop.words,nomatch=FALSE) %>% # remove stop words\n#         `!` %>%                             # remove stop words\n#         { word.vector[.] } %>%              # remove stop words\n#         ngrams(ngram.vector) %>%            # create n-grams\n#         sapply(function(word.vec) {         # create n-gram words\n#           paste(word.vec,collapse=\".\") \n#         }) %>%\n#         paste(collapse=\" \")                 # create string of n-grams\n#     }) %>%\n#     as.character() \n# }\n\n# removes columns from dataset\nreduce.dtm = function (dtm, freq.threshold) {\n  word.counts=colSums(dtm)\n  new.columns = names(word.counts)[freq.threshold<=word.counts]\n  dtm[,new.columns]\n}\n\n# List the ten most common words in cluster i\nTopWords = function (dtm, clusters, i) { \n  dtm_names = colnames(dtm)\n  row_count = sum(clusters==i)\n  dtm_csums =\n    apply(matrix(dtm[clusters==i,], nrow=row_count),\n          2,\n          mean)\n  names(dtm_csums) = dtm_names\n  dtm_ndx = order(dtm_csums, decreasing=TRUE)[1:10]\n  bind_rows(\n    data.frame(word=paste(c(\"[cluster \",\n                            formatC(i, format=\"f\", digits=0),\n                            \"]\"), \n                          collapse=\"\"),\n               avg=NA),\n    data.frame(word=paste(c(\"[\",\n                            formatC(row_count, format=\"f\", digits=0),\n                            \" records]\"), \n                          collapse=\"\"),\n               avg=NA),\n    data.frame(word=dtm_names[dtm_ndx], \n               avg=dtm_csums[dtm_ndx])\n  )\n}\n\ncheck.clusters = function(cluster, count.min) { \n  cluster.counts = table(cluster)\n  as.numeric(names(cluster.counts)[cluster.counts >= count.min]) %>%\n    lapply(function(clnum) { # clnum=1\n      TopWords(dtm,cluster,clnum) \n    }) %>%\n    bind_cols()\n} \n\nview.dtm = function(cluster.number) {\n  docs[res$cluster==cluster.number]\n}\nview.cluster = function(cluster.number) {\n  docs[cluster==cluster.number]\n}\n\n#\n# Define functions (END)",
    "created" : 1491688869023.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3609781849",
    "id" : "E722DB95",
    "lastKnownWriteTime" : 1490717679,
    "last_content_update" : 1490717679,
    "path" : "~/Desktop/Bentley/Data Mining (MA710)/Text Mining NY Times/20170321_TextMining_functions.R",
    "project_path" : "20170321_TextMining_functions.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}