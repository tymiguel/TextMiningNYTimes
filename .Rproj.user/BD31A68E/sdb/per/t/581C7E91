{
    "collab_server" : "",
    "contents" : "# Text mining and topic analysis of New York Times articles\n\n# Make sure you run the commands in the \n# file 20151104_TextMining_functions.R\n# before you run these commands. \n\n# Load the required libraries\nlibrary(cluster)\nlibrary(RCurl)\nlibrary(RJSONIO)\nlibrary(rlist)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(RTextTools)\nlibrary(ngram)\n\nsource(file='~/ /20170321_TextMining_fuctions.R')\n\n# Set this so we can see all columns \n# when printing dataframes\noptions(dplyr.width=Inf)\n\n# Replace the following key with your own, \n# which you can obtain from the New York Times\n# developer site http://developer.nytimes.com\narticlesearch.key = \"d19f24bdfbd940c69deaf0922e521eef\"\n\n# The code that you will modify to\n# create your clusters starts here.\n\n# OPTIONS: query, begin and end dates\n# Create a dataframe of articles from the New York Times\nget.nyt.hits(query.string=\"dance\",     # OPTION\n             begin.date=\"20170101\",    # OPTION\n             end.date  =\"20170107\")    # OPTION\n# each page carries 10 articles\n\narticle.df = get.nyt.articles(pages = -1,                 # all articles, can also change to 1:3\n                              query.string = \"dance\",     # OPTION\n                              begin.date   = \"20170101\",  # OPTION\n                              end.date     = \"20170107\")  # OPTION\ndim(article.df)\n\n# Check number of articles returned\nnum.articles = nrow(article.df)\nnum.articles\n\n# Check a random sample of 5 articles \ndoc.ndx = sample(1:num.articles,5)\narticle.df[doc.ndx,]\n\n# OPTION: headline, snippet, lead_paragraph or abstract\n# Create `docs` (the document vector) by\n# choosing the text field that you will analyze.\ndocs = article.df$snippet \n\n# Check a few of the documents\ndocs[doc.ndx]\n# These same documents will be checked below\n# after other modifications to the documents\n\n# Remove punctuation and numbers.\n# OPTION: you may find it useful to\n# change the cleaning procedure and\n# modify the function `clean.documents`.\ndocs.clean = clean.documents(docs)\n\nsave(docs.clean, \n     file=\"~/Desktop/Bentley/Data Mining (MA710)/Assignment 3/docs.clean.RData\")\nload(file=\"~/Desktop/Bentley/Data Mining (MA710)/Assignment 3/docs.clean.RData\")\n\n# Check the cleaned documents\ndocs.clean[doc.ndx]\n\n# OPTIONS: see code below\n# Modify the words in the documents \n# with stemming, n-grams and stopwords\ndocs.sns = \n  modify.words(\n    docs.clean,  \n    stem.words=FALSE,  # OPTION: TRUE or FALSE\n    ngram.vector=1:2, # OPTION: n-gram lengths\n    stop.words=       # OPTION: stop words\n      c(stopwords(kind=\"english\")  \n        # OPTION: \"SMART\" or \"english\" \n        # OPTION: additional stop words\n      )\n  )\n\n# Be careful: some stop words from the \n# stopwords function might be important \n# For example, \"new\"\n# \"new\" %in% stop.words # \"new york\", \"new england\" and \"new hampshire\" \n\n# Check documents\ndocs.sns[doc.ndx]\n\n# OPTION: weighting, see below\n# Create the document matrix\ndoc.matrix <- \n  create_matrix(docs.sns, # \"sns\" stem ngrams stop\n                language=\"english\",      # Do not change / we want english\n                stemWords=FALSE,         # Do not change / we already did this\n                removePunctuation=FALSE, # Do not change / we already did this\n                weighting=tm::weightTf   # OPTION: weighting (see below)\n  )\n# Weighting OPTIONS:\n# tm::weightTfIdf - term frequency-inverse document frequency\n# tm::weightTf    - term frequency\n# To use binary weighting use tm::weightTf and \n# create a \"binary matrix\" below.\n                  \n# Check the document matrix\ndoc.matrix\n\n# OPTIONS: none, but this command must be run\n# Create the document-term matrix\ndtm = as.matrix(doc.matrix) \n\n# Check the matrix\ndtm[1:10,1:10]\ndim(dtm) \n\n# Check the number of words in \n# the document term matrix\ncolnames(dtm)\nncol(dtm)\n\n# Check the distribution of document-word frequencies \ntable(dtm) # counts the 0s and the 1s\n\n# OPTION: create a binary matrix\n# in order to use binary weighting.\n# DO NOT run this code if you \n# DO NOT want to use binary weighting.\n# Only use with parameter\n#     weighting=tm::weightTf \n# All positive frequencies become 1,\n# indicating only the presence of a word  \n# in a  document. \n# Uncomment the following line to use\n# this code if you decide to use it. \n# dtm[dtm>1]=1 # turn each cell that is greater than 1, to a 1\n\n# This may not make much of a difference \n# as nearly all document-word frequencies \n# are equal to 1. Most duplicate words are\n# stopwords, and those have been removed. \n\n# Check the distribution of document-word frequencies \n# if you created a binary document-word matrix above\ntable(dtm)\n\n# Check the distribution of word frequencies \ntable(colSums(dtm)) #1200 words occured in 1 document, #97 occured in 2 documents \n# This gives the distribution of word frequencies \n# for the entire collection of articles\n\n# OPTION: frequency threshold\n# Keep words from the document term matrix\n# that occur at least the number of times\n# indicated by the `freq.threshold` parameter \ndtm=reduce.dtm(dtm,freq.threshold=2) # only keep the columns where the column sum is greater \n# the more dimensions, the max distance is just larger between two data points \n# this is important to remember when doing cluster analysis\n\n# Check the number of columns/words \n# remaining in the document-term matrix\nncol(dtm)\n\n# OPTION: number of clusters to find\nk = 3\n\n# OPTION: cluster algorithm \ncluster = kmeans(dtm,k)$cluster\n# cluster = pam(dtm,k)$cluster\n# hclust.res = hclust(dist(dtm))\n# cluster = cutree(hclust.res,k)\n\n# EVALUATE the clusters using `table` \n# to check the cluster sizes\nas.data.frame(table(cluster)) # does a frequency of the clusters # our clusters suck\n\n\n# EVALUATE the clusters using `check.cluster` \n# to look at the common words in each cluster\n# The second parameter is the minimum number of\n# rows that a cluster must have to be displayed.\noptions(warn=-1)\ncheck.clusters(cluster,5) \n# gives you cluster, how many records, then the value of the word, dance got 0.09 / 9%\n# we are averaging the columns, so dance shows up in 9$\n# binary, we will see proportion\n# if it's term frequeny, we can get more than 1\n# you don't want to look at clusters that are two small\noptions(warn=0)\n\n# EVALUATE the clusters using `TopWords` \n# This is the same information as supplied\n# by the `check.cluster` function, except \n# that the output is displayed vertically\noptions(warn=-1)\n1:k %>%\n  lapply(function(i) TopWords(dtm, cluster, i))\noptions(warn=0)\n\n# EVALUATE the clusters by looking \n# at the documents in the clusters\nview.cluster(3)\n\n# End\n",
    "created" : 1490547250552.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "341470567",
    "id" : "581C7E91",
    "lastKnownWriteTime" : 1490135811,
    "last_content_update" : 1490135811,
    "path" : "~/Desktop/Bentley/Data Mining (MA710)/Text Mining NY Times/20170321_TextMining_analysis.R",
    "project_path" : "20170321_TextMining_analysis.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}